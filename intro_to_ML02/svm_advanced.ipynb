{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Advanced SVM with Face detection\n",
    "\n",
    "Now let's try something more challenging: facial detection.\n",
    "\n",
    "SVMs maximise the margin between the support vectors.\n",
    "\n",
    "References:\n",
    "Faces recognition example using eigenfaces and SVMs: https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.fixes import loguniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the LFW dataset\n",
    "\n",
    "The labelled faces in the wild (LFW) dataset is famous for testing facial recognition algorithms. Faces are easy for humans to identify but notoriously difficult for machines. The LFW dataset provides a fantastic mix of celebrities faces in all sorts of lighting conditions, angles, and facial expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LWF dataset for classes with at least 50 samples\n",
    "\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "\n",
    "\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Load the LWF dataset for classes with at least 50 samples\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=50, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some examples of what these photos look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "def title(y, target_names, i):\n",
    "    return target_names[y[i]].rsplit(\" \", 1)[-1]\n",
    "\n",
    "prediction_titles = [\n",
    "    title(y, target_names, i) for i in range(y.shape[0])\n",
    "]\n",
    "\n",
    "plot_gallery(X, prediction_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the dataset's complexity\n",
    "\n",
    "Notice that there are more features than samples? This is a **very** bad sign as it tells us we don't have enough training data!\n",
    "\n",
    "Hence, we'll need to do some preprocessing to simplify the dataset's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "\n",
    "# Let's use z-scaling to ensure no features dominate others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Let's use z-scaling to ensure no features dominate others\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with z-scaling, the numerous features will be too much to work with.\n",
    "\n",
    "What we need is to reduce the number of features. This presents a problem: *which features do we drop?*\n",
    "\n",
    "Thankfully, scikit-learn already provides an algorithm for combining features: *Principal Component Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of components\n",
    "# It should be high enough to avoid losing too much information\n",
    "# but not too high to prevent the curse of dimensionality\n",
    "# n_components = \n",
    "\n",
    "# Create the PCA analyser and train it\n",
    "\n",
    "\n",
    "# Transform the training and testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Choose a number of components\n",
    "# It should be high enough to avoid losing too much information\n",
    "# but not too high to prevent the curse of dimensionality\n",
    "n_components = 150\n",
    "\n",
    "# Create the PCA analyser and train it\n",
    "pca = PCA(n_components=n_components).fit(X_train)\n",
    "\n",
    "# Transform the training and testing data\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what these eigenfaces look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our classifier\n",
    "\n",
    "Due to the high number of features, we'll use SVM.\n",
    "\n",
    "However, we don't know what our hyperparameters should be. Furthermore, there's **way** too many possible combinations of hyperparameters to comb through. Instead, we'll do a gridsearch to speed this process up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your grid of varying C and gamma values\n",
    "\n",
    "\n",
    "# Create the model using SVC and RandomizedSearchCV\n",
    "# Use a RBF kernel and balanced class weights\n",
    "\n",
    "\n",
    "# Train the model using the PCA data\n",
    "\n",
    "\n",
    "# Display the best model found\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Define your grid of varying C and gamma values\n",
    "param_grid = {\n",
    "    \"C\": loguniform(1e3, 1e5),\n",
    "    \"gamma\": loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "# Create the model using SVC and RandomizedSearchCV\n",
    "# Use a RBF kernel and balanced class weights\n",
    "model = RandomizedSearchCV(\n",
    "    SVC(kernel='rbf', class_weight='balanced'),\n",
    "    param_grid, n_iter=10\n",
    ")\n",
    "\n",
    "# Train the model using the PCA data\n",
    "model2 = model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Dispaly the best model found\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(model.best_estimator_)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix of our model\n",
    "\n",
    "With our best model known, let's see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's predictions with the PCA test data\n",
    "\n",
    "\n",
    "# Plot the confustion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model, X_test_pca, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Get the model's predictions with the PCA test data\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model, X_test_pca, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final time, we'll look at our performance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e8ba394dfecf69ab3b214a23e7e279956d3407d86cf3cc4fe5cfb6d1f0e6e68"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
