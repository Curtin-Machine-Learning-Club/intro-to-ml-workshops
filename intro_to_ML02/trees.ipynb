{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Decision Trees and Random Forest\n",
    "\n",
    "We all know what a decision tree is, but how do we build one optimally?\n",
    "\n",
    "Answer: we use Gini impurities or entropy/information gain for our splits.\n",
    "\n",
    "Decisions trees are simple to understand and visualise, which makes them perfect for your non-tech savvy boss and clients to grasp.\n",
    "\n",
    "References:\n",
    "Decision trees: https://scikit-learn.org/stable/modules/tree.html <br>\n",
    "Plot the decision surface of decision trees trained on the iris dataset: https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html <br>\n",
    "Plot the decision surfaces of ensembles of trees on the iris dataset: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html <br>\n",
    "Confusion matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install sklearn\n",
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the Iris dataset\n",
    "\n",
    "The iris dataset is one of these most famous 20th century datasets for ML. Made up of 150 samples of three types of flowers (50 per type), with four features of sepal and petal width and length. The Iris dataset is simple to understand with plenty of variance, overlap, and some outliers, making it perfect for testing and showcasing ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Iris dataset\n",
    "\n",
    "Let's view the dataset one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_x, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "iris = datasets.load_iris() # Needed for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing our inputs\n",
    "\n",
    "Scikit-learn requires our input data to be numpy arrays, so let's convert them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into numpy arrays\n",
    "X = iris_x.to_numpy()\n",
    "y = iris_y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Convert the data into numpy arrays\n",
    "X = iris_x.to_numpy()\n",
    "y = iris_y.to_numpy()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Decision tree\n",
    "\n",
    "Let's make one classifier for every unique pair of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of the pairs of features\n",
    "pairIdxs = [[0, 1],\n",
    "            [0, 2],\n",
    "            [0, 3],\n",
    "            [1, 2],\n",
    "            [1, 3],\n",
    "            [2, 3]]\n",
    "\n",
    "# Choose your max depth\n",
    "# max_depth = \n",
    "\n",
    "# Create the decision tree models for each pair\n",
    "\n",
    "\n",
    "# Train the models in a collection called models2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Indexes of the pairs of features\n",
    "pairIdxs = [[0, 1],\n",
    "            [0, 2],\n",
    "            [0, 3],\n",
    "            [1, 2],\n",
    "            [1, 3],\n",
    "            [2, 3]]\n",
    "\n",
    "# Choose your max depth\n",
    "max_depth = 15\n",
    "\n",
    "# Create the decision tree models for each pair\n",
    "models = [DecisionTreeClassifier(max_depth=max_depth) for _ in pairIdxs]\n",
    "\n",
    "# Train the models\n",
    "models2 = [model.fit(X[:, pair], y) for model, pair in zip(models, pairIdxs)]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "pairIdxs = [[0, 1],\n",
    "            [0, 2],\n",
    "            [0, 3],\n",
    "            [1, 2],\n",
    "            [1, 3],\n",
    "            [2, 3]]\n",
    "\n",
    "fig, sub = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "\n",
    "for pairidx, (model, pair, ax) in enumerate(zip(models2, pairIdxs, sub.flatten())):\n",
    "    # For plotting purposes, we only include two features\n",
    "    X2 = X[:, pair]\n",
    "\n",
    "    x_min, x_max = X2[:, 0].min() - 0.3, X2[:, 0].max() + 0.3\n",
    "    y_min, y_max = X2[:, 1].min() - 0.3, X2[:, 1].max() + 0.3\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    "    )\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    X_pred = np.zeros((len(xx.ravel()), 2))\n",
    "    X_pred[:, 0] = xx.ravel()\n",
    "    X_pred[:, 1] = yy.ravel()\n",
    "\n",
    "    Z = model.predict(X_pred)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    ax.set_xlabel(iris.feature_names[pair[0]])\n",
    "    ax.set_ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(len(iris.target_names)), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        ax.scatter(\n",
    "            X2[idx, 0],\n",
    "            X2[idx, 1],\n",
    "            c=color,\n",
    "            label=iris.target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15,\n",
    "        )\n",
    "\n",
    "plt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Visualisation\n",
    "\n",
    "Scikit-learn comes with a handy tool to actually show what our decision tree looks like. Below is what the tree looks like when trained on all four features. Unfortunately, the picture is usually very pixelated and difficult to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure()\n",
    "model = DecisionTreeClassifier().fit(iris.data, iris.target)\n",
    "plot_tree(model, filled=True)\n",
    "plt.title(\"Decision tree trained on all the iris features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's use the `graphviz` package to visualise our tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "dot_data = tree.export_graphviz(model, out_file=None, \n",
    "    feature_names=iris.feature_names,  \n",
    "    class_names=iris.target_names,  \n",
    "    filled=True, rounded=True,  \n",
    "    special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the image to a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "\n",
    "dot_data = tree.export_graphviz(model, out_file=None, \n",
    "    feature_names=iris.feature_names,  \n",
    "    class_names=iris.target_names,\n",
    "    filled=True, rounded=True,  \n",
    "    special_characters=True)  \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "\n",
    "# pick a model from our trained models\n",
    "# model = models2[0]\n",
    "\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        display_labels=iris.target_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits dataset\n",
    "\n",
    "Decisions trees are nice and simple, but they tend to overfit the training data very badly.\n",
    "\n",
    "Instead, we can use a random forest of decision trees to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "First things first, we need to load the data. Let's also view the first few samples while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits as a bunch object\n",
    "# We do this to get the target names and images for plotting\n",
    "\n",
    "# Also load the digits X as a pandas Dataframe and the y as a Series\n",
    "\n",
    "\n",
    "# Plot the first few examples\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Digit {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Load the digits as a bunch object\n",
    "# We do this to get the target names and images for plotting\n",
    "digits = datasets.load_digits()\n",
    "# Also load the digits X as a pandas Dataframe and the y as a Series\n",
    "digits_X, digits_y = datasets.load_digits(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Plot the first few examples\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Digit {label}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split our labelled data into training and testing sets with a 70/30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits_X.to_numpy(), digits_y.to_numpy(), train_size=0.7\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters\n",
    "# n_trees = \n",
    "# criterion = 'gini', 'entropy'\n",
    "# max_depth = \n",
    "# max_features = 'auto', 'sqrt', 'log'\n",
    "\n",
    "# Create the untrained model with your hyperparameters\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "# get the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click to cheat</summary>\n",
    "\n",
    "```python\n",
    "# Define your hyperparameters\n",
    "n_trees = 100\n",
    "criterion='gini'\n",
    "max_depth=5\n",
    "max_features='auto'\n",
    "\n",
    "# Create the untrained model with your hyperparameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=n_trees,\n",
    "    criterion=criterion,\n",
    "    max_depth=max_depth,\n",
    "    max_features=max_features\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Let's see a few examples of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, prediction in zip(axes, X_test, y_pred):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also view our confusion matrix for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at our measures of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = [str(name) for name in digits.target_names]\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e8ba394dfecf69ab3b214a23e7e279956d3407d86cf3cc4fe5cfb6d1f0e6e68"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
